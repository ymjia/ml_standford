---
title: summary
header-includes:
  - \usepackage{cleveref}
author: jiayanming
---

## 目标函数Hypothesis
当遇到较复杂的问题时，我们希望通过已有的数据总结规律，进而利用此规律对新的数据作出判断。例如我们想对当前地区的房子进行估价，房子的价值与其一些固有属性有关，如面积，户型，通勤等等。即，对于一个代表房子属性的向量$m^{(i)} = [size, layout, commute, ...]$，其价值是 $p^{(i)}$。

我们的目标是找到一个函数映射来近似表示这一规律，由$m^{(i)}$映射到$p^{(i)} = h_{\Theta}(m^{(i)})$

为此，我们对数据集进行简单的分析，然后假定一个目标函数形式，如：
$h_{\Theta}(m^{(i)}) = \theta_0 + \theta_1 * m^{(i)}_1 + \theta_2 * (m^{(i)}_2)^2 + ...$
其中，上标(i)是训练集元素的id，下标是训练向量中的元素id。

利用已有数据（训练集）对函数进行“训练”，训练目标是：获得一组参数$\Theta = [\theta_1, \theta_2, ..., \theta_n]$，使目标函数的输出结果尽可能“符合”训练集，然后可利用得到的$\Theta$代入到目标函数 $h_{\Theta}$对新数据进行预测。

## 损失函数Cost Function
有了目标函数后，为了找到最优的参数，我们需要对目标函数与训练集是否“符合”进行评估，显然，对于不同的 $\\Theta$ ，目标函数在训练集上的表现不相同，为此引入一个关于 $\Theta$ 的函数：损失函数$J(\Theta)$，用来表征目标函数$h_{\Theta}(m^{(i)})$的性能。对于训练集中的训练数据，利用损失函数评估当前参数下目标函数的性能。此时，我们训练的目的可以更明确的表示为：对损失函数做优化，寻找令损失函数取得最小值的$\Theta$。

## 梯度下降法 Gradient descent
由于损失函数有不同的类型，可能是线性的也可能是非线性的，对其进行优化有可能很复杂，为此我们引入一种迭代优化方法，使其能够对大部分损失函数有较好的优化结果，这一方法叫作梯度下降法。

对于一个可导函数$f(x)$，我们在求解其极大/极小值时，通常会对函数进行求导得到$f'(x)$，然后取其导数为0的点即为极值点。类似的，我们用迭代的方式获取损失函数$J(\Theta)$的极值点，步骤如下：

1. 给定一个初始化参数值 $\Theta^{(0)} = [\theta_0, \theta_1, ..., \theta_n]$，对应的损失函数值是$J(\Theta_0)$

2. 在局部范围内改变$\Theta = \Theta + \Delta\Theta$，使得$J(\Theta)$减小

3. 迭代改变$\Theta$，直到$J(\Theta)$值收敛。

以上过程中，存在两个问题：

1. 迭代过程如何收敛。

   通过“精心挑选”损失函数的定义形式，我们可以保证损失函数是一个凸函数，此时，通过不断迭代，可以保证收敛到函数的最值。

2. 如何确定$\Delta\Theta$使得$J(\Theta)$值在每次迭代中变小。

   根据函数梯度的定义，函数 $J(\Theta)$ 在 $\Theta$ 点的梯度方向为函数值增加最快的方向，因此我们取
$$\Delta\Theta = -\lambda * grad(J(\Theta))$$
其中 $grad(J)$ 表示函数 $J$ 在 $\Theta$ 的梯度，$\lambda$ 是迭代步长。在每次迭代中，$J(\Theta)$ 沿其梯度的反方向移动一小段距离，在 $\lambda$ 取值合适的前提下，因为损失函数 $J(\Theta)$ 是凸函数，我们可以通过足够的迭代次数，找到使 $J(\Theta)$ 取得最小值的 $\Theta$ 。

根据以上步骤，损失函数$J(\Theta)$的值沿其梯度的反方向逐步下降，所以此方法称为梯度下降法。接下来我们在具体的机器学习算法中，观察目标函数、损失函数、梯度下降法是如何解决实际问题的。

## 线性回归问题(拟合问题) Linear Regression


## 逻辑回归问题(分类问题) Logistic Regression

## 神经网络 Neral Network

